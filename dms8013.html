<!DOCTYPE html>
<! Site built using W3 CSS tutorials, embedresponsively.com and favicon-generator.org >
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href='https://fonts.googleapis.com/css?family=Roboto+Condensed|Satisfy|Open+Sans:700' rel='stylesheet' type='text/css'>
<link rel="apple-touch-icon" sizes="57x57" href="/apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="/apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="/apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="/apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="/apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="/apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="/apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="/apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="/apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="/android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="/favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="/ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">
<meta name="author" content="Meena Daneshyar">
<meta name="description" content="Portfolio of Meena Daneshyar, a Digital Artist and Musician based in Newcastle upon Tyne, UK.">
<meta name="keywords" content="meena, daneshyar, digital, artist, art, newcastle, upon, tyne, creative, practice, code, processing, musician, music, singer, Newcastle upon Tyne, Durham">

<style>
* {
    box-sizing: border-box;
	margin-top: 0px;
    margin-bottom: 0px;
    margin-right: 0px;
    margin-left: 0px;
}

[class*="col-"] {
    width: 100%;
	float: left;
	padding-top 3em;
	padding-left: 2em;
	padding-right: 2em;
	padding-bottom: 3em;
}
@media only screen and (min-width: 768px) {
    /* For desktop: */
    .col-1 {width: 25%;}
    .col-2 {width: 50%;}
    .col-3 {width: 75%;}
	.col-4 {width: 100%;}
}

img {
    max-width: 80%;
    height: auto;
	border: 0px solid
}

hr {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), #B2DFDB, rgba(0, 0, 0, 0));
}


h1 {
	border-style: none;
	width:100%;
	font-family: 'Satisfy', cursive;
	font-size: 4em;
	color: #FFFFFF;
	font-style: normal;
	text-align: center;
	background-color: #00796B;
	text-shadow: 0px 1px 2px #B2DFDB;
	line-height: 100%;
	padding-top:0.5em;
	padding-bottom:0.5em;
	margin-top: 0px;
    margin-bottom: 0px;
    margin-right: 0px;
    margin-left: 0px;
}

h2 {
	border-style: none;
	width:100%;
	font-family: "Open Sans", sans-serif;
	font-size: 1em;
	font-weight: normal;
	font-style: normal;
	letter-spacing:0.2em;
	text-transform: uppercase;
	text-align: center;
	background-color: #B2DFDB;
	color: #212121;
	margin-top: 0px;
    margin-bottom: 0em;
    margin-right: 0px;
    margin-left: 0px;
	padding-top:0.5em;
	padding-bottom:0.5em;
}

strong {
	border-style: none;
	font-family: "Open Sans", sans-serif;
	font-size: 1em;
	font-weight: normal;
	font-style: normal;
	letter-spacing:0.2em;
	text-transform: uppercase;
	text-align: center;
	color: #212121;
}

body {
	background-color: white;
	font-family: "Roboto Condensed";
	color: #212121;
	font-size: 1em;
	text-align: center;
	font-weight: normal;
	font-style: normal;
	text-transform: none;
	padding:none;
}

div.fixed {
	position: fixed;
	bottom: 0.2em;
	right: 0.2em;
	width: 100px;
	border: none;
	font-size:0.7em;
	color:#212121;
	background-color:none;
}

a:link, a:visited, a:hover, a:active {
	text-decoration: none;
	color:#212121;
}

ul {
	display: inline-block;
    list-style-type: none;
    margin: 0;
    padding: 0;
    overflow: hidden;	
    padding-bottom: 1em;
}

li {
    float: left;
	align: center;
	border-right: 1px solid #B2DFDB;
}

li a {
    display: block;
    color: #212121;
    text-align: center;
    padding: 8px 10px;
    text-decoration: none;	
	border-style: none;
	width:100%;
	font-family: "Open Sans", sans-serif;
	font-size: 0.7em;
	font-weight: normal;
	font-style: normal;
	letter-spacing:0.2em;
	text-transform: uppercase;
	text-align: center;
	margin-top: 0px;
    margin-bottom: 0em;
    margin-right: 0px;
    margin-left: 0px;
	padding-top:0.5em;
	padding-bottom:0.5em;
	align: center;
}

li a:hover {
    background-color: #B2DFDB;
}

li a.active {
    background-color: #B2DFDB;
}

div#nav{
  text-align: center;
  width:100%;
}

li:last-child {
    border-right: none;
}

</style>
<title>Meena Daneshyar - Portfolio</title>
</head>
<a id = "top"></a>
<h1>Meena Daneshyar.</h1>
<h2>Portfolio</h2>
<body>
<div class="nav">
<ul>
  <li><a href="index.html" alt="About Me">About</a></li>
  <li><a href="http://dm.ncl.ac.uk/meenadaneshyar/" alt="Blog">Blog</a></li>
  <li><a href="dms8012.html" alt="DMS8012: Advanced Creative Digital Practice: Live Electronic Performance">DMS8012</a></li>
  <li><a class="active" href="dms8013.html" alt="DMS8013: Digital Media: Advanced Theory and Practice">DMS8013</a></li>
  <li><a href="http://github.com/meenadaneshyar/" alt="Github">Github</a></li>
</ul>
</div>
<div class="col-4" ><hr><strong>DMS8013: Digital Media: Advanced Theory and Practice</strong><hr></div>
<div class="col-4">In this portfolio I explore 3 different developed pieces of code which I have drawn from my engagement with the DMS8013 module "Digital Media: Advanced Theory and Practice". Two of these explore the use of analysing audio, using amplitude and frequency as a source of data. The third is analysing video as a source of data, the techniques employed following on from a lecture that was given as part of the course. I talk in more detail about how each of these were developed and the common themes throughout these projects; interactivity, migration, engagement of audience curiosity and transformation of physical embodiment.</div>
<div class="col-4"><hr></div>
	<div class="col-2"><strong>Project Amplitude</strong><br><br> For code, see <a href="https://github.com/MeenaDaneshyar/Amplitude-moving-grass">Github.</a> <br><br> For this project I utilised both Processing and Pure Data. The Pure Data patch is using the computer microphone as an input, and analysing the amplitude. This data is then sent via Open Sound Control (OSC) to the Processing sketch. The Processing sketch draws a simulation of a field of grass in 3D, and the data received from Pure Data is applied to the Z axis of the tips of the grass, each blade bending away from the screen in relation to the increase of the amplitude.<br><br>
To create this piece I went through a series of steps, initially finding an example of measuring amplitude using an Arduino and a sound module. From this, I changed the amplitude analysis calculations to be done in Pure Data rather than Arduino as Pure Data is better suited for audio based work. Using this data collected by Pure Data, I manipulated a sketch that was previously made in a lecture (building a solar system model in 3D space). The amplitude was made to control the distance of an orbiting planet relative to the sun. I then built upon the idea of producing reactive 3D animations by writing a new sketch for the visuals, creating an array of curved lines to simulate grass in the wind. The interaction now has a new meaning, not only being triggered by making a loud noise, but can also being triggered by someone blowing on the microphone, simulating wind blowing through the grass. <br><br>
The project’s use of interactivity is displayed in a very literal sense; you can see a direct correlation between making a loud noise and a change in the visuals. However, the perceived cause and effect of an image of grass moving would indicate a gust of wind. This then contradicts the socially normal perceived effect from an increase in volume and instead simulates a degree of wind intensity in the 3D space. The grass can also be seen as analogous to hairs on a human head; where the normal reaction to a loud, frightening noise would be to make the hair stand on end, this sketch does the opposite. The project also touches on migration – the movement of one form into another. It displays how audio data can be manipulated and moved into the realm of image data. Another theme within the project is that of transformation of physical embodiment; there is an engagement between the body and the computer which transforms the physical phenomenon of noise or wind into a digital space which gives visual feedback to the audience, via an arbitrary relationship. <br><br>
This code as it stands is just in a prototype, proof-of-concept stage. The final piece would be intended to be displayed in a gallery setting. This would be presented as a large projection on a wall with boundary microphones placed adjacent to the screen.  The concept would touch on another running theme in this piece; because of the setting, it engages with the curiosity of the audience as a sound will have to be made at a consistent level before they become aware of how their interaction changes what they see. The nature of the interaction would not be immediately obvious, leading to a surprise when the reactive nature of the piece is noticed.<br><br>
Throughout the course I have been shown many of the basics of the Processing language, and with my background in music, I decided to explore different ways I can detect audio and connect this with different types of visuals. This ties in to the development of my MA major project. <br><br>
Through developing this sketch I have gained a greater understanding of 3D space in Processing, the positioning along the X, Y and Z axis as well as mapping between variables. I have also learnt how to connect processing to Pure Data through OSC network messages, a useful tool in developing sound-based pieces.<br><br>
</div>
<div class="col-2">
<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style><div class='embed-container'><iframe src='https://player.vimeo.com/video/166528730' frameborder='0' webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe></div>
<br>
Video showing work in progress sketch  - planet position responding to amplitude.
<br><br><br>
<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style><div class='embed-container'><iframe src='https://player.vimeo.com/video/166518836' frameborder='0' webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe></div>
<br>
Video showing final Processing sketch.
</div>
<div class = "col-4"><hr></div>
<div class="col-2"><strong>Project FFT</strong><br><br> For code, see <a href="https://github.com/MeenaDaneshyar/FFT-to-Light">Github.</a><br><br><a href = "https://commons.wikimedia.org/wiki/File:Inverse_visible_spectrum.svg?uselang=en-gb">Source of image used and license information.</a><br><br>For this project, it was necessary to utilise only the Processing environment. The sketch is analysing the audio input from the computer microphone and performing a Fast Fourier Transform (FFT) to detect the different component frequencies of the input signal. Then using this data, Processing manipulates it to represent the spectrum of visible light. Since the visible light spectrum is a series of different frequencies, a mapping is performed between the sound frequencies and the frequencies of light. The resulting colour is then displayed on the screen. The FFT analysis looks at all the frequencies which make up the input signal; this means this piece will work from any audio input, including singing and whistling as well as just speech itself. <br><br>
In order to create this piece I went through different examples such as “FFTtransforms” to understand how to use FFT. This then was developed into a sketch which depending on the peak frequency of the signal, moves forward and backwards through a downloaded gif animation (high frequencies move forward, low frequencies move backwards). After this sketch, I looked into the different wavelengths (and therefore frequencies) of light and developed the code from the gif sketch to obtain the most significant frequency and map it onto an image of the light spectrum, from which Processing then extracts the relevant colour information. The interactivity of the project is not dissimilar to the amplitude project explained above. The physical act of changing pitch shifts up and down the light spectrum, however this relationship would not be obvious to the audience without some continuous level of interactivity or a previous introduction to the piece. This engagement required from the audience draws from, and is vital to, the audience’s curiosity in the piece; the audience member will not actually expect or see anything happen unless they first make a noise (predicted to be unintentional).  The technology used in this project like Project Amplitude shows a migration from one sensory perception to another; moving sound from audio to visuals. This is also a transformation from the body, and its physical audibility, into a visual response from the computer, through the medium of light. This ties in to wider themes of the relationships between human sensory perceptions, such as the condition of Synaesthesia, where some individuals are known to experience sound as colour. <br><br>
In order for this to be improved I would add in extra components such as LED’s to be respond to the light spectrum displayed, rather than have the colour shown on the screen of the laptop.  These would be controlled via an Arduino which would communicate with the computer over a Serial interface.<br><br>
This project is a prototype for an exhibition piece. The project’s presentation will be a dark room where there are positioned lights and the audience is free to enter and walk around the space. The frequencies picked up by microphones around the space would then trigger the different coloured lights positioned around the edges of the room, leaving the audience to experience the colours from the different light wave frequencies according the their own sound frequencies. This also has the potential to be a performance piece with the use of different selected songs or filmed scenes/ dialogue as the audio input to control the light frequency, creating a re-enactment of the given audio input through the light spectrum.<br><br>
I chose to do this project because the knowledge I learned about sound frequencies made me think about the different types of waves, such as those that make up light, and how frequency manifests itself in them. As they both have the form of a wave form, a collaboration of the two seems inevitable.<br><br>
From this project I learned about FFT analysis, which also resulted in learning the basic physics of sound wave forms, including the principle of superposition. I also have learned about light waves and the different ways colour can be represented in a computer.  I have also learned about how to work with images and how to extract raw colour data from individual pixels. These skills will be useful when working with colour in future projects, as well as when wanting to work with frequencies of sound.<br><br>
<br><br>
</div>
<div class="col-2">
<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style><div class='embed-container'><iframe src='https://player.vimeo.com/video/166525781' frameborder='0' webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe></div>
<br> Video showing work in progress sketch - playing a gif animation forwards or backwards depending on frequency.
<br><br><br>
<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style><div class='embed-container'><iframe src='https://player.vimeo.com/video/166521675' frameborder='0' webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe></div>
<br>
Video showing final Processing sketch.
</div>
<div class = "col-4"><hr></div>
	<div class="col-2"><strong>Project Surveillance</strong><br><br>For code, see <a href="https://github.com/MeenaDaneshyar/Surveillance-project">Github.</a><br><br><a href = "https://commons.wikimedia.org/wiki/File:Theresa_May_-_Home_Secretary_and_minister_for_women_and_equality.jpg?uselang=en-gb">Source of image used and license information.</a><br><br>This project uses Processing to detect faces from a live camera feed. The sketch then displays an animated pair of eyes which sit behind a still image of Theresa May (the current Home Secretary); the eyes are tracking the detected faces so as to appear to follow you around the room as you move within the camera peripherals.<br><br>
When making this project I looked at the OpenCV library in Processing, looking at the “FaceDetect” example. This project follows on from work that was started in a lecture where I worked on the combination of the “FaceDetection” example and the “GettingStartedCapture” example in the video library, to detect faces in a video stream. After being given the task to swap detected faces on the screen, I went away and expanded this sketch, after which I gained more of an understanding of how Processing uses video and images and how it stores pixel information. As I have a sketch which can detect where the face is in a video, I decided to see if I could position visuals in the same direction to the detected face. Using the example “ToonShading” in the “Shaders” folder of Processing examples as inspiration, I proceeded to make a pair of eyes on the screen follow the position of the mouse first, using a map function. Later, I applied the face detection method to this. The interactivity with this project is unlike the previous two projects as this interactivity is engaged unconsciously. The concept of always being watched through the use of surveillance cameras plays amusingly with the basic human instinct of survival and superstition behind the sensation of being watched.  This ideology of an Orwellian surveillance society has been used within popular culture throughout the 21st century such as the “Big Brother” TV show (based on an omnipresent character in Orwell’s “1984”). Surveillance propaganda has become a hot topic in recent years through exposés such as the NSA being revealed to monitor the public’s conversations as track their movements via mobile phones. This project presents the subject comically with the character established as the Home Secretary Theresa May, who is known for her dedicated involvement with writing the so-called “Snooper’s Charter”, a law to give the government more access to the public within the private sphere. This blunt representation is what engages the audience’s curiosity into the project, taking an opposite effect to the previous two pieces. Unlike the other two projects, project surveillance has a more physical relationship to the software, transforming the motion of the real-world body to the movement of a computational representation of a body part.<br><br>
Unfortunately the OpenCV library that I am using for this sketch is not always very reliable, often mistaking spaces in the video for another face. This then makes it difficult for the eyes to follow one person within the frame, and may require further tweaking. <br><br>
Project Surveillance is at a prototype stage, and there are multiple ways in which it could be presented. One way would be to have the finished piece shown on a large screen overlooking a busy metropolitan city. This again would then be subject to unconscious interactivity as well as less engagement with the curiosity of the “audience” as they are not actively pushed towards viewing the piece as would be in  gallery and are instead much more passive. This however reinforces the “not knowing you are being watched” concept and the lackadaisical reaction that many members of the public have to surveillance. The other way in which this could be presented would be as an exhibition piece that is a projection on a wall within a gallery – this would give the piece much more attention and active engagement due to the environment it is in. <br><br>
The inspiration from this piece came from exploring Processing libraries and finding one which draws a cartoon-shaded sphere that tilts in the direction of the mouse. This got me thinking about the classic memorable scenes from the children’s TV show “Scooby Doo” where in a haunted place the portraits on the wall would watch them – showing the eyes move. From this I decided to make my own as I had already accomplished face detection in a camera feed, and it only seemed appropriate to use the image of Theresa May for this purpose.<br><br>
After making this piece I am better at mapping co-ordinate points within the space of Processing – making it easier to understand how the use of space in Processing works. I am also more skilled at using external Processing libraries and integrating these with my other skills.<br><br>
</div>
<div class="col-2"><style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style><div class='embed-container'><iframe src='https://player.vimeo.com/video/166499906' frameborder='0' webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe></div><br>Video showing the initial sketch functionality which tracks mouse movement. Slow performance is due to the use of the "blur" filter which was later removed.<br><br><br><style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style><div class='embed-container'><iframe src='https://player.vimeo.com/video/166499943' frameborder='0' webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe></div><br>Video showing the sketch above with the image positioned on top.<br><br><br>
<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style><div class='embed-container'><iframe src='https://player.vimeo.com/video/166527049' frameborder='0' webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe></div>
<br>
</div>
<div class="col-4"><hr></div>
<div class=col-4"><strong>Sources used</strong><br><br>
<a href = "http://www.theguardian.com/uk-news/2016/mar/01/snoopers-charter-to-extend-police-access-to-phone-and-internet-data">Article on the "Snooper’s Charter", Alan Travis, 01/03/2016</a><br>
<a href = "https://www.youtube.com/watch?v=I9db3hmA96U">PD Tutorial on smoothing values, Dr. Rafael Hernandez, 5/12/2009</a><br>
<a href = "https://commons.wikimedia.org/wiki/File:Inverse_visible_spectrum.svg">Visible light spectrum image, "Gringer", 30/08/2008</a><br>
<a href = "https://stackoverflow.com/questions/3407942/rgb-values-of-visible-spectrum">StackOverflow discussion on converting visible light frequencies to RGB, Various, 2010</a><br>
<a href = "https://commons.wikimedia.org/wiki/File:Theresa_May_-_Home_Secretary_and_minister_for_women_and_equality.jpg?uselang=en-gb">Portrait of Theresa May, UK Home Office, 17 May 2010</a><br>
</div>
<div class = "col-4"><hr></div>
<div class="fixed"><a href="#top">Back to top</a></div>
</body>